---
kind: article
title: Makefiles
---

Any bioinformatics research project should be reproducible. This means that
that anyone should be able to repeat all the steps in a project, ideally using
a single command. This allows anyone else interested in the research to see
what processes were used to produce the results. Build files can to manage
these dependencies and go a long way making this reproducibility possible.

[GNU Make][make] provides a syntax to describes dependency steps as targets in
a computational workflow. Each Make target generates a file as the input of the
next target. A target is repeated if the file generated by the previous target
has a time stamp later than the current target file or if the file does not
exist. An example Makefile looks like this:

[make]: ADD LINK TO MAKE

    two.txt: one.txt
        # Run command line steps here

    one.txt:
        # Run command line steps here

Each task is named and dependencies are listed after the colon. The required
commands are described on subsequent indented lines beneath the task name.
These commands should generate the file named in the task line as subsequent
tasks will depend on this.

The example Makefile above defines two tasks to generate the files: one.txt and
two.txt. In this workflow the generation of the file 'two.txt' is dependent on
the generation of 'one.txt.' Calling `make` at the command line will cause make
to run through the target in the order defined in the Makefile.

I have since started using GNU Make in my computational projects. This is
because it provides a simple syntax to described the steps in a workflow.
Furthermore the dependency based system ensures that previous targets are
refreshed if become out-of-sync to the workflow process. Overall this can make
computational projects very reproducible as I will outline in the following
sections.

### Write build files agnostic of a specific language

When I wrote the original ['Organised Bioinformatics Experiments'][organised]
article I programmed almost exclusively in Ruby with the exception of R for
terminal analysis target. Using Rake was ideal as I could write my workflow
processes in my favorite programming language.

In the last two years I've been experimenting with Erlang and Clojure. I would
now like to include these languages in my workflow in addition to Ruby. The
advantage of using Make instead of Rake is that every project is that every
target is simply a call to the shell.

Therefore where of all analysis code was Ruby written inside a Rakefile, I
instead package up the analysis code, in any language, as a binary to call from
the Makefile. Separating the analysis targets into individual files is
particularly suited to using Make as I'll outline in the next section.
Furthermore separating each analysis into an individual file makes it much
easier to switch out one language for another when appropriate. I think this
will become more and more common in future as more and more niche-specific
languages become available. Finally as every Makefile target uses the shell it
makes it much easier to use the very powerful coreutils available on every UNIX
based system.

[organised]: LINK TO ORGANISED ARTICLE

### Generative code as a target dependency

Any part file can be a dependency to a Make task, including the files doing the
processing in the analysis step. For example, consider the slightly more complex
target:

    output.txt: bin/analyse input.txt
        $^ > $@

The target is a file called **output.txt** which requires two dependencies:
**bin/analyse** and **input.txt**. The executed code however uses two Makefile
macros. Macros can be used to simplify writing Makefiles. In this case **$^**
expands to the list of dependencies and **$@** expands to the target file. This
would therefore is the same as writing:

    output.txt: bin/analyse input.txt
       bin/analyse input.txt > output.txt

The advantage of making 'bin/analyse' a target dependency is that whenever this
file is edited this file is that the target of this step and any downstream
target will be rerun when I call `make` again. This clarifies a critical part of
making workflow reproducible: it is not only when input data changes that a
workflow needs to be rerun, but also any scripts that process the data also.

### Abstraction of steps

Consider a more complex example with three Makefile targets:

    all: prot00001.txt prot00002.txt prot00003.txt prot00004.txt

    *.txt: ./bin/intensive_operation *.fasta
       $^ > $@

    *.fasta:
        curl http://database.example.com/$@ > $@

TODO: UPDATE WITH THE CORRECT * MACRO

The line is the target **all** depending on four files: prot0000[1-4].txt. This
target however executes no code and instead insures only that the four
dependency files are generated. There are however no tasks to generate these
four files individually, instead there are two tasks to generate files based on
two possible file type extensions: **.txt** and **.fasta**. These wild card
operators will be expanded to fill in the required parts for each dependency.
For instance the following targets would be executed for the first target:

    prot00001.txt: ./bin/intensive_operation prot00001.fasta
        ./bin/intensive_operation prot00001.fasta > prot00001.txt

    prot00001.fasta:
        curl http://database.example.com/prot00001.fasta > prot00001.fasta

This wild card \* format allows the process to be abstracted out independent of
the individual files to be generated. This makes it much simpler to change the
data analysed whilst still maintaining the same workflow. Concretely,
additional files to the `all` task without changing the workflow process.
Idiomatically, I should use a variable named `objects` to specify the output I
am interested in.

    objects = prot00001.txt prot00002.txt prot00003.txt prot00004.txt

    all: $(objects)

    *.txt: ./bin/intensive_operation *.fasta
       $^ > $@

    *.fasta:
        curl http://database.example.com/$@ > $@

I would go further than this however and move all the objects to a separate
file and read their names in as a dependency. This would mean you would only
change the Makefile when making changes to your workflow, thereby isolating
changes between the workflow process and the workflow output to different files
in the revision control history.

###### Very simple parallelisation

Bigger data sets required more time to process and taking advantage of multi-core
processors is a cheap way to reduce this running time. Given the Makefile I
outlined in the previous section I can run this command.

  `make -j 4`

This create a separate process to generate each of the four required files. If
the number of files required to be generated exceeds the number of processes
specified then these will begin after another has finished. This is a very
cheap method to add multi-core parallelisation to a workflow, as long as you can
abstract out the workflow processes to fit this.
