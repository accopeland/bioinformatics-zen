---
kind: article
title: Makefiles
---

A bioinformatics research project should be easily reproducible. Anyone should
be able to repeat all the steps from scratch and the simpler this can be done
the better. Reproducibility allows anyone to see the processes used to generate
the results. Build files can be used to manage the dependencies between project
steps and go a long way allowing reproducible results.

[GNU Make][make] provides a syntax to describe project dependency steps as
targets in a computational work flow. Each target is a file created from as the
input of the next target. A target is repeated if the file generated by the
previous target has a time stamp later than the current target file or if the
file does not exist. An example Makefile looks like this:

[make]: ADD LINK TO MAKE

    two.txt: one.txt
        # Run command line steps here

    one.txt:
        # Run command line steps here

Each task is named and dependencies are listed after the colon ':'. The
required commands are described on the indented lines following the task name.
These commands should generate the file named in the task if any further tasks
use this as a dependency.

The Makefile example above defines two tasks, each generating a files: either
one.txt or two.txt. In this workflow the generation of the file 'two.txt' is
dependent on the generation of 'one.txt.' Calling `make` at the command line
will evaluate these targets in the order defined in the Makefile.

I have since started using GNU Make in my computational projects because the
syntax also very simple specification of a workflow. Furthermore the dependency
system ensures that each targets is regenerated if any early steps change.
Overall this can make computational projects very reproducible. For the rest of
this post I'm going describe the advantages of this approach in futher detail.

### Build workflows independent of any specific language

When I wrote my original post on ['Organised Bioinformatics
Experiments'][organised] I was programmed the majority of my time in Ruby.
Therefore using Ruby-based Rake tool to build my workflows was ideal as I could
write in my favorite programming language.

In the last two years however I've been experimenting with more functional
programming languages such as Erlang and Clojure. I feel that in future I will
continue to broaden the languages I use in different contexts and therefore
include these languages in my workflows. The advantage of using Make instead of
Rake is that every workflow step is simply a call to the shell and rather than
being tied to a specific programming language.

In my my current process I package up the analysis code, in any language, as a
binary to call from the Makefile. Separating each analysis step into a single
file makes it much easier to switch out one language for another when
appropriate. Often I end up replacing scripts with the shell based pipelines
using a combination of pipes and coreutils which is even simpler than writing
your own script. Finally separating the analysis targets into individual files
allows an even more powerful aspect for creating more reproducible
computational workflows as I'll outline in the following section. 

[organised]: LINK TO ORGANISED ARTICLE

### Generative code as a target dependency

Any file can be a dependency to a Make task, including the files doing the
processing in the analysis step. For example, consider the example Makefile
target:

    output.txt: bin/analyse input.txt
        $^ > $@

The target is a file called **output.txt** which requires two dependencies:
**bin/analyse** and **input.txt**. The executed code however uses two Makefile
macros. Macros can be used to simplify writing Makefiles. In this case **$^**
expands to the list of dependencies and **$@** expands to the target file. This
therefore is similar to writing:

    output.txt: bin/analyse input.txt
       bin/analyse input.txt > output.txt

The advantage of making 'bin/analyse' a target dependency is that whenever this
file is edited the target of this step and any downstream target will be
regenerated when I call `make` again. This clarifies a critical part of making
workflow reproducible: it is not only when input data changes that a workflow
needs to be rerun, but also any scripts that process or generate data.

### Abstraction of steps

Consider this example with three Makefile targets:

    all: prot00001.txt prot00002.txt prot00003.txt prot00004.txt

    *.txt: ./bin/intensive_operation *.fasta
       $^ > $@

    *.fasta:
        curl http://database.example.com/$@ > $@

TODO: UPDATE WITH THE CORRECT * MACRO

Here the target **all** depending on four files: prot0000[1-4].txt. This target
executes no code but instead insures only that the four dependency files are
generated. Looking through the following lines you can see that there are no
tasks to generate these four files individually. Instead there are tasks to
generate files based on the filetype extensions: either **.txt** or **.fasta**.
When running `make` These wild card operators will be expanded to fill in the
required parts for each dependency. For instance the following steps would be
executed for the first target and so forth for each file specified in the
**all** step:

    prot00001.txt: ./bin/intensive_operation prot00001.fasta
        ./bin/intensive_operation prot00001.fasta > prot00001.txt

    prot00001.fasta:
        curl http://database.example.com/prot00001.fasta > prot00001.fasta

This wild card \* format allows the process to be abstracted out independent of
the individual files to be generated. This makes it much simpler to change the
data analysed whilst still maintaining the same workflow. Concretely, this
means you can specify additional files to be generate in the *all* task without
changing the rest of the workflow. Idiomatically, I should use a variable named
**objects** to specify the output I am interested in.

    objects = prot00001.txt prot00002.txt prot00003.txt prot00004.txt

    all: $(objects)

    *.txt: ./bin/intensive_operation *.fasta
       $^ > $@

    *.fasta:
        curl http://database.example.com/$@ > $@

You could go even further than this however and move the list of target files
to a separate file and read their names in as a dependency. This would mean you
would only change the Makefile when making changes to your workflow, thereby
isolating changes between the workflow process and the workflow output to
different files in the revision control history.

### Very simple parallelisation

Bigger data sets required more time to process and taking advantage of
multi-core processors is a cheap way to reduce this. Given the Makefile I
outlined in the previous section using filetype rules I can invoke make as
follows.

  `make -j 4`

This create a separate process to generate each of the required files. If the
number of files required to be generated exceeds the number of processes
specified then these will begin after another has finished. This is a very
cheap method to add multi-core parallelisation to a workflow, as long as you
can abstract out the workflow processes to fit this.
