---
  kind: article
  title: Reproducible software benchmarks
  created_at: "2009-01-26 00:00 GMT"
---

New bioinformatics software is continuous produced and published. The constant
stream of new releases makes it difficult to keep track of improvements to
algorithms for common bioinformatics tasks. An example of this problem is the
domain of genome assembly where there is already a large number of existing
software and new approaches are being published.

When researching which bioinformatics software to use in for data, there is the
problem understanding how effective the software is. For example given a new
paper published on a genome assembler, how can you know how well it will
assemble your data? A publication may include benchmarks these may not be
applicable to different data, or may suffer from the author's bias.

Even if you know which is the best genome assembler for your data, another
question is how easy is it to install? Does the software require complex
dependencies to install? Or does it fail inexplicably with obscure error
messages? The poor usability experience of many bioinformatics software can
lead to scientists using the software that the colleague sitting next to them
knows how to install and debug, rather than what may be the state of the art.

## A registry of assemblers

There is precedent for solving these problems and previous approaches are:

    * The recent [Assemblathon 1][asm1] and [Assemblathon 2][asm2] aimed to
      evaluate the current state of genome assembly. Their approach was to
      release a set of genome reads and ask the community to submit their best
      assembly. The quality and accuracy of each of the submitted assemblies
      was then evaluated and compared with each other.

   *  The [Genome Assembly Gold-Standard Evaluations (GAGE)][gage] took the
      opposite approach and instead themselves ran several genome assemblers
      against four different read datasets. The results of each assembler were
      then evaluated for performance.

I believe that these approaches are the right direction for the bioinformatics
community to move in. The development of objective performance benchmarks, not
just for genome assembly but for all common tasks, would help researchers which
is the best software and which they should be using in their own research. I
believe that these approaches can be taken a step further to solve the problems
I described above. For instance:

  * Create a registry of available assemblers that is continuously updated as
    new assemblers become available and are published. This can act as a
    centralised repository for all available genome assemblers. Anyone can and
    should register their assembler with this database. This would then allow
    researchers to browse current available assemblers.

  * Continuously benchmark software against test datasets. This would attempt
    to provide an objective evaluation of each assembler's effectiveness.
    Furthermore by running these benchmarks on a regular and reoccurring basis
    new developments could be continuously evaluated.

  * Simple installation and configuration of parameters. The assembler
    developer should specify the configuration of the assembler's parameters
    for most common scenarios. Furthermore the installation of the assembler
    should be as simple as possible, allowing a low barrier of entry for use.

To this end I created the website [nucleotides - a registry of genome
assemblers and benchmarks][nuc].

## Reproducible genome assembly benchmarks using Docker

The key to this website is that all assemblers are constructed as
[Docker][docker] images. If you are unfamiliar with Docker, an image is
analogous to a list of instructions or blueprint that specifies how an assembler
should be installed and used. If a system has Docker installed, this blueprint
(called a Dockerfile) can be used to install everything required to get an
assembler running. This thereby simplifies installation for an assembler, each
assembler can installed on a system with a `docker pull` pull command.

If assemblers are packaged up as Docker images using a common API then running
benchmarks against a variety of assemblers is much simpler. Each assembler can
be cloned from the repository and then given a test data set. The output of the
assembler can then be compared against the reference genome to give a benchmark
of how well the assembler performed. This is the homepage of
[nucleotid.es][nuc] where each benchmark is calculated using [quast][] against
the reference. Using a variety of test datasets it is possible for a use to see
which assembler may work best for their own data.

[asm1]: http://www.ncbi.nlm.nih.gov/pubmed/21926179

[asm2]: http://www.ncbi.nlm.nih.gov/pubmed/23870653

[gage]: http://www.ncbi.nlm.nih.gov/pubmed/22147368

[nuc]: http://nucleotid.es

[lxc]: https://linuxcontainers.org/

[docker]: http://www.docker.com/

[quast]: http://www.ncbi.nlm.nih.gov/pubmed/23422339
