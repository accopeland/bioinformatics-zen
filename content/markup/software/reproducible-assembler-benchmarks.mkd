---
  kind: article
  title: Continuous, reproducible genome assembler benchmarking
  created_at: "2009-01-26 00:00 GMT"
---

New bioinformatics software is continuous produced and published. The constant
stream of new releases makes it difficult to keep track of improvements to
algorithms for common bioinformatics tasks. An example of this problem is the
domain of genome assembly where there is already a large number of existing
software and new approaches are being published.

When researching which bioinformatics software to use in for data, there is the
problem understanding how effective the software is. For example given a new
paper published on a genome assembler, how can you know how well it will
assemble your data? A publication may include benchmarks these may not be
applicable to different data, or may suffer from the author's bias.

Even if you know which is the best genome assembler for your data, another
question is how easy is it to install? Does the software require complex
dependencies to install? Or does it fail inexplicably with obscure error
messages? The poor usability experience of many bioinformatics software can
lead to scientists using the software that the colleague sitting next to them
knows how to install and debug, rather than what may be the state of the art.

## A registry of assemblers

There is precedent for solving these problems and previous approaches are:

    * The recent [Assemblathon 1][asm1] and [Assemblathon 2][asm2] aimed to
      evaluate the current state of genome assembly. Their approach was to
      release a set of genome reads and ask the community to submit their best
      assembly. The quality and accuracy of each of the submitted assemblies
      was then evaluated and compared with each other.

   *  The [Genome Assembly Gold-Standard Evaluations (GAGE)][gage] took the
      opposite approach and instead themselves ran several genome assemblers
      against four different read datasets. The results of each assembler were
      then evaluated for performance.

I believe that these approaches are critical for the bioinformatics community.
The development of objective performance benchmarks, not just for genome
assembly but for all common tasks, helps researchers determine which software
they should be using in their research. I believe that these approaches can be
taken a step further to solve the problems I described above. For instance:

  * Create a registry of available assemblers that is continuously updated as
    new assemblers become available. This acts as a centralised repository for
    all available genome assemblers. Anyone can and should register their
    assembler with this database. This would then allow researchers to browse
    all available assemblers in a single place.

  * Continuously benchmark the assemblers against reference data. This would
    provide an objective evaluation of each assembler's effectiveness for a
    given data set. Furthermore running these benchmarks regularly allows new
    assemblers to be evaluated and compared against existing benchmarks. This
    allow researchers to compare which genome assembler performs best for a
    given type of data, e.g. a low GC content genome, and use the best
    performing for their own data.

  * Provide simple installation and configuration of parameters. The assembler
    developer provides an out-of-the-box version of the assembler that includes
    defaults parameters that works for most common scenarios. Furthermore the
    installation of the assembler should be as simple as possible, allowing a
    low barrier of entry for use. This allows a researcher to use an assembler
    without requiring difficult of manual installation of software.

I created the website [nucleotides - a registry of genome assemblers and
benchmarks][nuc] with the end of satisfying all of these goals..

## Reproducible genome assembly benchmarks using Docker

The key to this website is that all assemblers are constructed as
[Docker][docker] images. If you are unfamiliar with Docker, an image is
analogous to a list of instructions or blueprint that specifies how an assembler
should be installed and used. If a system has Docker installed, this blueprint
(called a Dockerfile) can be used to install everything required to get an
assembler running. This thereby simplifies installation for an assembler, each
assembler can installed on a system with a `docker pull` pull command.

If assemblers are packaged up as Docker images using a common API then running
benchmarks against a variety of assemblers is much simpler. Each assembler can
be cloned from the repository and then given a test data set. The output of the
assembler can then be compared against the reference genome to give a benchmark
of how well the assembler performed. This is the homepage of
[nucleotid.es][nuc] where each benchmark is calculated using [quast][] against
the reference. Using a variety of test datasets it is possible for a use to see
which assembler may work best for their own data.

[asm1]: http://www.ncbi.nlm.nih.gov/pubmed/21926179

[asm2]: http://www.ncbi.nlm.nih.gov/pubmed/23870653

[gage]: http://www.ncbi.nlm.nih.gov/pubmed/22147368

[nuc]: http://nucleotid.es

[lxc]: https://linuxcontainers.org/

[docker]: http://www.docker.com/

[quast]: http://www.ncbi.nlm.nih.gov/pubmed/23422339
